{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ML TEXT PREPROCESSING USIN SPARK\n",
    "##### @ author : Frederic TWAHIRWA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is advantageous for text analytics because it provides a platform for scalable, distributed computing.\n",
    "\n",
    "When it comes to text analytics, you have a few option for analyzing text. I like to categorize these techniques like this:\n",
    "\n",
    "    - Text Mining (i.e. Text clustering, data-driven topics)\n",
    "    - Categorization (i.e. Tagging unstructured data into categories and sub-categories; hierarchies; taxonomies)\n",
    "    - Entity Extraction (i.e. Extracting patterns such as phrases, addresses, product codes, phone numbers, etc.)\n",
    "    - Sentiment Analysis (i.e. Tagging positive, negative, or neutral with varying levels of sentiment)\n",
    "    - Deep Linguistics (i.e Semantics. Understanding causality, purpose, time, etc.) \n",
    "\n",
    "Which technique you use typically depends on the business use case and the question(s) you are trying to answer. It's also common to combine these techniques.\n",
    "\n",
    "### This post will focus on feature Engineering for Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.Builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.Builder().getOrCreate() # required for dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text pre-processing\n",
    "### create a dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dataFrame\n",
    "inputDF = spark.createDataFrame([(0, \"This is a ML project. A ML is a software project, not a hardware project\"),\n",
    "                                 (1, \"Software project is best described as a collection of software programs \"),\n",
    "                                 (2, \"Hardware project is best described as a device\")],\n",
    "                                  [\"id\", \"document\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This is a ML project. A ML is a software proje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Software project is best described as a collec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hardware project is best described as a device</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           document\n",
       "0   0  This is a ML project. A ML is a software proje...\n",
       "1   1  Software project is best described as a collec...\n",
       "2   2     Hardware project is best described as a device"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputDF.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Tokenization is the process of taking text (such as a sentence) and breaking it into individual terms (usually words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------------------+\n",
      "|id |document                                                                |\n",
      "+---+------------------------------------------------------------------------+\n",
      "|0  |This is a ML project. A ML is a software project, not a hardware project|\n",
      "|1  |Software project is best described as a collection of software programs |\n",
      "|2  |Hardware project is best described as a device                          |\n",
      "+---+------------------------------------------------------------------------+\n",
      "\n",
      "+---+----------------------------------------------------------------------------------------+\n",
      "|id |words                                                                                   |\n",
      "+---+----------------------------------------------------------------------------------------+\n",
      "|0  |[this, is, a, ml, project., a, ml, is, a, software, project,, not, a, hardware, project]|\n",
      "|1  |[software, project, is, best, described, as, a, collection, of, software, programs]     |\n",
      "|2  |[hardware, project, is, best, described, as, a, device]                                 |\n",
      "+---+----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol = \"document\", outputCol = \"words\")\n",
    "\n",
    "tokenizedDF = tokenizer.transform(inputDF)\n",
    "tokenizedDF.select('id', 'document').show(truncate = False)\n",
    "tokenizedDF.select('id', 'words').show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ponctuation removal ( using regexTokenizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------------------+\n",
      "|id |document                                                                |\n",
      "+---+------------------------------------------------------------------------+\n",
      "|0  |This is a ML project. A ML is a software project, not a hardware project|\n",
      "|1  |Software project is best described as a collection of software programs |\n",
      "|2  |Hardware project is best described as a device                          |\n",
      "+---+------------------------------------------------------------------------+\n",
      "\n",
      "+---+--------------------------------------------------------------------------------------+\n",
      "|id |words                                                                                 |\n",
      "+---+--------------------------------------------------------------------------------------+\n",
      "|0  |[this, is, a, ml, project, a, ml, is, a, software, project, not, a, hardware, project]|\n",
      "|1  |[software, project, is, best, described, as, a, collection, of, software, programs]   |\n",
      "|2  |[hardware, project, is, best, described, as, a, device]                               |\n",
      "+---+--------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol = \"document\", outputCol = \"words\", pattern = \"\\\\s+|,|\\\\.\")\n",
    "\n",
    "tokenizedDF = regexTokenizer.transform(inputDF)\n",
    "tokenizedDF.select('id', 'document').show(truncate = False)\n",
    "tokenizedDF.select('id', 'words').show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of the StopWords\n",
    "Stop words are words which should be excluded from the input, typically because the words appear frequently and don’t carry as much meaning.\n",
    "\n",
    "We are going to use \"StopWordsRemover\".StopWordsRemover takes as input a sequence of strings (e.g. the output of a Tokenizer) and drops all the stop words from the input sequences. The list of stopwords is specified by the stopWords parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwordsRemover = StopWordsRemover(inputCol = \"words\", outputCol = \"words_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', \"i'll\", \"you'll\", \"he'll\", \"she'll\", \"we'll\", \"they'll\", \"i'd\", \"you'd\", \"he'd\", \"she'd\", \"we'd\", \"they'd\", \"i'm\", \"you're\", \"he's\", \"she's\", \"it's\", \"we're\", \"they're\", \"i've\", \"we've\", \"you've\", \"they've\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"haven't\", \"hasn't\", \"hadn't\", \"don't\", \"doesn't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"mustn't\", \"can't\", \"couldn't\", 'cannot', 'could', \"here's\", \"how's\", \"let's\", 'ought', \"that's\", \"there's\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", 'would']\n"
     ]
    }
   ],
   "source": [
    "print (stopwordsRemover.loadDefaultStopWords('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|            document|               words|      words_filtered|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  0|This is a ML proj...|[this, is, a, ml,...|[ml, project, ml,...|\n",
      "|  1|Software project ...|[software, projec...|[software, projec...|\n",
      "|  2|Hardware project ...|[hardware, projec...|[hardware, projec...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "removedDF = stopwordsRemover.transform(tokenizedDF)\n",
    "removedDF.show(truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+--------------------------------------------------------------------+\n",
      "|document                                                                |words_filtered                                                      |\n",
      "+------------------------------------------------------------------------+--------------------------------------------------------------------+\n",
      "|This is a ML project. A ML is a software project, not a hardware project|[ml, project, ml, software, project, hardware, project]             |\n",
      "|Software project is best described as a collection of software programs |[software, project, best, described, collection, software, programs]|\n",
      "|Hardware project is best described as a device                          |[hardware, project, best, described, device]                        |\n",
      "+------------------------------------------------------------------------+--------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "removedDF.select(\"document\", \"words_filtered\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate n-grams\n",
    "An n-gram is a sequence of n tokens (typically words) for some integer n. The NGram class can be used to transform input features into n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram = NGram(n = 2, inputCol = \"words\", outputCol = \"ngrams\")  # bigram\n",
    "ngramDF = ngram.transform(removedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a ML project. A ML is a software project, not a hardware project\n",
      "['this is', 'is a', 'a ml', 'ml project', 'project a', 'a ml', 'ml is', 'is a', 'a software', 'software project', 'project not', 'not a', 'a hardware', 'hardware project']\n"
     ]
    }
   ],
   "source": [
    "row = ngramDF.select(\"document\", \"ngrams\").collect()[0]\n",
    "\n",
    "print (row['document'])\n",
    "print (row['ngrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software project is best described as a collection of software programs \n",
      "['software project', 'project is', 'is best', 'best described', 'described as', 'as a', 'a collection', 'collection of', 'of software', 'software programs']\n"
     ]
    }
   ],
   "source": [
    "row = ngramDF.select(\"document\", \"ngrams\").collect()[1]\n",
    "\n",
    "print (row['document'])\n",
    "print (row['ngrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram = NGram(n = 3, inputCol = \"words\", outputCol = \"ngrams\")  # trigram\n",
    "ngramDF = ngram.transform(removedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a ML project. A ML is a software project, not a hardware project\n",
      "['this is a', 'is a ml', 'a ml project', 'ml project a', 'project a ml', 'a ml is', 'ml is a', 'is a software', 'a software project', 'software project not', 'project not a', 'not a hardware', 'a hardware project']\n"
     ]
    }
   ],
   "source": [
    "row = ngramDF.select(\"document\", \"ngrams\").collect()[0]\n",
    "\n",
    "print (row['document'])\n",
    "print (row['ngrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software project is best described as a collection of software programs \n",
      "['software project is', 'project is best', 'is best described', 'best described as', 'described as a', 'as a collection', 'a collection of', 'collection of software', 'of software programs']\n"
     ]
    }
   ],
   "source": [
    "row = ngramDF.select(\"document\", \"ngrams\").collect()[1]\n",
    "\n",
    "print (row['document'])\n",
    "print (row['ngrams'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF*DF\n",
    "Term frequency-inverse document frequency (TF-IDF) is a feature vectorization method widely used in text mining to reflect the importance of a term to a document in the corpus. Denote a term by t, a document by d, and the corpus by D. Term frequency TF(t,d) is the number of times that term t appears in document d, while document frequency DF(t,D) is the number of documents that contains term t. If we only use term frequency to measure the importance, it is very easy to over-emphasize terms that appear very often but carry little information about the document, e.g. “a”, “the”, and “of”. If a term appears very often across the corpus, it means it doesn’t carry special information about a particular document. Inverse document frequency is a numerical measure of how much information a term provides:\n",
    "\n",
    "$$\n",
    "IDF(t,D)= log\\frac{|D|+1}{DF(t,D)+1},\n",
    "$$\n",
    "\n",
    "where |D| is the total number of documents in the corpus. Since logarithm is used, if a term appears in all documents, its IDF value becomes 0. Note that a smoothing term is applied to avoid dividing by zero for terms outside the corpus. The TF-IDF measure is simply the product of TF and IDF:\n",
    "\n",
    "$$\n",
    "TFIDF(t,d,D)=TF(t,d)⋅IDF(t,D).\n",
    "$$\n",
    "\n",
    "There are several variants on the definition of term frequency and document frequency. In MLlib, we separate TF and IDF to make them flexible\n",
    "\n",
    "more info : https://spark.apache.org/docs/2.2.0/ml-features.html#tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer    # implemented as an estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#countVectorizer = CountVectorizer(inputCol = \"words_filtered\", outputCol = \"features_tf\", vocabSize = 2)\n",
    "countVectorizer = CountVectorizer(inputCol = \"words_filtered\", outputCol = \"features_tf\") \n",
    "model = countVectorizer.fit(removedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'software', 'described', 'hardware', 'ml', 'best', 'programs', 'device', 'collection']\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print (model.vocabulary)\n",
    "print (len(model.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countDF = model.transform(removedDF)\n",
    "row = countDF.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a ML project. A ML is a software project, not a hardware project\n",
      "['ml', 'project', 'ml', 'software', 'project', 'hardware', 'project']\n",
      "(9,[0,1,3,4],[3.0,1.0,1.0,2.0])\n"
     ]
    }
   ],
   "source": [
    "print (row['document'])\n",
    "print (row['words_filtered'])\n",
    "print (row['features_tf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.linalg.SparseVector"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(row['features_tf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = row['features_tf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.,  1.,  0.,  1.,  2.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF  # implemented as an estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol = \"features_tf\", outputCol = \"features_tf_idf\")\n",
    "idfModel = idf.fit(countDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------------------+\n",
      "|id |document                                                                |\n",
      "+---+------------------------------------------------------------------------+\n",
      "|0  |This is a ML project. A ML is a software project, not a hardware project|\n",
      "|1  |Software project is best described as a collection of software programs |\n",
      "|2  |Hardware project is best described as a device                          |\n",
      "+---+------------------------------------------------------------------------+\n",
      "\n",
      "+---+------------------------------------------------------------------------------------------------------------------------+\n",
      "|id |features_tf_idf                                                                                                         |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |(9,[0,1,3,4],[0.0,0.28768207245178085,0.28768207245178085,1.3862943611198906])                                          |\n",
      "|1  |(9,[0,1,2,5,6,8],[0.0,0.5753641449035617,0.28768207245178085,0.28768207245178085,0.6931471805599453,0.6931471805599453])|\n",
      "|2  |(9,[0,2,3,5,7],[0.0,0.28768207245178085,0.28768207245178085,0.28768207245178085,0.6931471805599453])                    |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featuresDF = idfModel.transform(countDF)\n",
    "featuresDF.select(\"id\", \"document\").show(truncate = False)\n",
    "featuresDF.select(\"id\", \"features_tf_idf\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = featuresDF.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a ML project. A ML is a software project, not a hardware project\n",
      "['ml', 'project', 'ml', 'software', 'project', 'hardware', 'project']\n",
      "(9,[0,1,3,4],[3.0,1.0,1.0,2.0])\n",
      "(9,[0,1,3,4],[0.0,0.287682072452,0.287682072452,1.38629436112])\n"
     ]
    }
   ],
   "source": [
    "print (row['document'])\n",
    "print (row['words_filtered'])\n",
    "print (row['features_tf'])\n",
    "print (row['features_tf_idf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = featuresDF.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a ML project. A ML is a software project, not a hardware project\n",
      "['ml', 'project', 'ml', 'software', 'project', 'hardware', 'project']\n",
      "(9,[0,1,3,4],[3.0,1.0,1.0,2.0])\n",
      "(9,[0,1,3,4],[0.0,0.287682072452,0.287682072452,1.38629436112])\n"
     ]
    }
   ],
   "source": [
    "print (row['document'])\n",
    "print (row['words_filtered'])\n",
    "print (row['features_tf'])\n",
    "print (row['features_tf_idf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features\n",
    "**StringIndexer** encodes a string column of labels to a column of label indices.\n",
    "\n",
    "** One-hot encoding** maps a column of label indices to a column of binary vectors, with at most a single one-value. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "catDF = spark.createDataFrame([\n",
    "    (0, \"Milan\"),\n",
    "    (1, \"London\"),\n",
    "    (2, \"Brussels\"),\n",
    "    (3, \"Milan\"),\n",
    "    (4, \"Paris\"),\n",
    "    (5, \"Paris\"),\n",
    "    (6, \"Milan\"),\n",
    "    (7, \"Brussels\")],\n",
    "    [\"row_id\", \"city\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+\n",
      "|row_id|    city|cityIndex|\n",
      "+------+--------+---------+\n",
      "|     0|   Milan|      0.0|\n",
      "|     1|  London|      3.0|\n",
      "|     2|Brussels|      1.0|\n",
      "|     3|   Milan|      0.0|\n",
      "|     4|   Paris|      2.0|\n",
      "|     5|   Paris|      2.0|\n",
      "|     6|   Milan|      0.0|\n",
      "|     7|Brussels|      1.0|\n",
      "+------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringIndexer = StringIndexer(inputCol = \"city\", outputCol = \"cityIndex\")\n",
    "model = stringIndexer.fit(catDF)\n",
    "indexedDF = model.transform(catDF)\n",
    "\n",
    "indexedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-------------+\n",
      "|row_id|    city|cityIndex|      cityVec|\n",
      "+------+--------+---------+-------------+\n",
      "|     0|   Milan|      0.0|(4,[0],[1.0])|\n",
      "|     1|  London|      3.0|(4,[3],[1.0])|\n",
      "|     2|Brussels|      1.0|(4,[1],[1.0])|\n",
      "|     3|   Milan|      0.0|(4,[0],[1.0])|\n",
      "|     4|   Paris|      2.0|(4,[2],[1.0])|\n",
      "|     5|   Paris|      2.0|(4,[2],[1.0])|\n",
      "|     6|   Milan|      0.0|(4,[0],[1.0])|\n",
      "|     7|Brussels|      1.0|(4,[1],[1.0])|\n",
      "+------+--------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# produce a vector with OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCol = \"cityIndex\", outputCol = \"cityVec\")\n",
    "encoder.setDropLast(False)\n",
    "encodedDF = encoder.transform(indexedDF)\n",
    "encodedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = encodedDF.collect()[0]\n",
    "row['cityVec'].toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = encodedDF.collect()[1]\n",
    "row['cityVec'].toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = encodedDF.collect()[2]\n",
    "row['cityVec'].toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1.,  0.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = encodedDF.collect()[4]\n",
    "row['cityVec'].toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "catDF = spark.createDataFrame([\n",
    "    (0, \"Milan\", \"fashion\"),\n",
    "    (1, \"London\", \"fashion\"),\n",
    "    (2, \"Brussels\", \"beer\"),\n",
    "    (3, \"Milan\", \"clothes\"),\n",
    "    (4, \"Paris\", \"wine\"),\n",
    "    (5, \"Paris\", \"books\"),\n",
    "    (6, \"Milan\", \"wine\"),\n",
    "    (7, \"Brussels\", \"books\")],\n",
    "    [\"row_id\", \"city\", \"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCol = \"city\", outputCol = \"cityIndex\")\n",
    "model = stringIndexer.fit(catDF)\n",
    "indexedDF = model.transform(catDF)\n",
    "\n",
    "stringIndexer2 = StringIndexer(inputCol = \"category\", outputCol = \"categoryIndex\")\n",
    "model2 = stringIndexer2.fit(indexedDF)\n",
    "indexedDF2 = model2.transform(indexedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+-------------+-------------+-------------+\n",
      "|city    |category|cityIndex|categoryIndex|cityVec      |categoryVec  |\n",
      "+--------+--------+---------+-------------+-------------+-------------+\n",
      "|Milan   |fashion |0.0      |0.0          |(4,[0],[1.0])|(5,[0],[1.0])|\n",
      "|London  |fashion |3.0      |0.0          |(4,[3],[1.0])|(5,[0],[1.0])|\n",
      "|Brussels|beer    |1.0      |3.0          |(4,[1],[1.0])|(5,[3],[1.0])|\n",
      "|Milan   |clothes |0.0      |4.0          |(4,[0],[1.0])|(5,[4],[1.0])|\n",
      "|Paris   |wine    |2.0      |2.0          |(4,[2],[1.0])|(5,[2],[1.0])|\n",
      "|Paris   |books   |2.0      |1.0          |(4,[2],[1.0])|(5,[1],[1.0])|\n",
      "|Milan   |wine    |0.0      |2.0          |(4,[0],[1.0])|(5,[2],[1.0])|\n",
      "|Brussels|books   |1.0      |1.0          |(4,[1],[1.0])|(5,[1],[1.0])|\n",
      "+--------+--------+---------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(inputCol = \"cityIndex\", outputCol = \"cityVec\")\n",
    "encoder.setDropLast(False)\n",
    "encodedDF = encoder.transform(indexedDF2)\n",
    "\n",
    "encoder2 = OneHotEncoder(inputCol = \"categoryIndex\", outputCol = \"categoryVec\")\n",
    "encoder2.setDropLast(False)\n",
    "encodedDF2 = encoder2.transform(encodedDF)\n",
    "encodedDF2.select('city', 'category', 'cityIndex', 'categoryIndex', 'cityVec', 'categoryVec').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols = [\"cityVec\", \"categoryVec\"],\n",
    "    outputCol = \"totalVec\")\n",
    "\n",
    "finalDF = assembler.transform(encodedDF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------------+-------------+-------------------+\n",
      "|    city|category|      cityVec|  categoryVec|           totalVec|\n",
      "+--------+--------+-------------+-------------+-------------------+\n",
      "|   Milan| fashion|(4,[0],[1.0])|(5,[0],[1.0])|(9,[0,4],[1.0,1.0])|\n",
      "|  London| fashion|(4,[3],[1.0])|(5,[0],[1.0])|(9,[3,4],[1.0,1.0])|\n",
      "|Brussels|    beer|(4,[1],[1.0])|(5,[3],[1.0])|(9,[1,7],[1.0,1.0])|\n",
      "|   Milan| clothes|(4,[0],[1.0])|(5,[4],[1.0])|(9,[0,8],[1.0,1.0])|\n",
      "|   Paris|    wine|(4,[2],[1.0])|(5,[2],[1.0])|(9,[2,6],[1.0,1.0])|\n",
      "|   Paris|   books|(4,[2],[1.0])|(5,[1],[1.0])|(9,[2,5],[1.0,1.0])|\n",
      "|   Milan|    wine|(4,[0],[1.0])|(5,[2],[1.0])|(9,[0,6],[1.0,1.0])|\n",
      "|Brussels|   books|(4,[1],[1.0])|(5,[1],[1.0])|(9,[1,5],[1.0,1.0])|\n",
      "+--------+--------+-------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalDF.select('city', 'category', 'cityVec', 'categoryVec', 'totalVec').show(truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
