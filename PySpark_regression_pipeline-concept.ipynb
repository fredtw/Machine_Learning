{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building A Linear Regression with PySpark and MLlib by using the pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@ author : Frederic Twahirwa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark has become one of the most commonly used and supported open-source tools for machine learning and data science. In this post, I’ll help you get started using Apache Spark’s spark.ml Linear Regression for predicting rental bike. Data is from \"UCI Machine learning repository : Bike sharing Data set\". In this example below we use the day data set.\n",
    "\n",
    "We are going to use the concept of pipeline in this example.\n",
    "In machine learning, it is common to run a sequence of algorithms to process and learn from data.MLlib represents such a workflow as a Pipeline, which consists of a sequence of PipelineStages (Transformers and Estimators) to be run in a specific order. We will use this simple workflow as a running example in example below.\n",
    "more info at https://spark.apache.org/docs/2.2.0/ml-pipeline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set : Attribute information\n",
    "\n",
    "- instant: record index\n",
    "- dteday : date\n",
    "- season : season (1:springer, 2:summer, 3:fall, 4:winter)\n",
    "- yr : year (0: 2011, 1:2012)\n",
    "- mnth : month ( 1 to 12)\n",
    "- hr : hour (0 to 23)\n",
    "- holiday : weather day is holiday or not (extracted from [Web Link])\n",
    "- weekday : day of the week\n",
    "- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "+ weathersit :\n",
    "- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "- temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\n",
    "- atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\n",
    "- hum: Normalized humidity. The values are divided to 100 (max)\n",
    "- windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
    "- casual: count of casual users\n",
    "- registered: count of registered users\n",
    "- cnt: count of total rental bikes including both casual and registered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import pyspark package, create a session and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>331</td>\n",
       "      <td>654</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>131</td>\n",
       "      <td>670</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>120</td>\n",
       "      <td>1229</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>108</td>\n",
       "      <td>1454</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.22927</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.1869</td>\n",
       "      <td>82</td>\n",
       "      <td>1518</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  instant      dteday season yr mnth holiday weekday workingday weathersit  \\\n",
       "0       1  2011-01-01      1  0    1       0       6          0          2   \n",
       "1       2  2011-01-02      1  0    1       0       0          0          2   \n",
       "2       3  2011-01-03      1  0    1       0       1          1          1   \n",
       "3       4  2011-01-04      1  0    1       0       2          1          1   \n",
       "4       5  2011-01-05      1  0    1       0       3          1          1   \n",
       "\n",
       "       temp     atemp       hum windspeed casual registered   cnt  \n",
       "0  0.344167  0.363625  0.805833  0.160446    331        654   985  \n",
       "1  0.363478  0.353739  0.696087  0.248539    131        670   801  \n",
       "2  0.196364  0.189405  0.437273  0.248309    120       1229  1349  \n",
       "3       0.2  0.212122  0.590435  0.160296    108       1454  1562  \n",
       "4  0.226957   0.22927  0.436957    0.1869     82       1518  1600  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_sharing = spark_session.read.csv(\"./Documents/MachineLearning/BikeSharing/day.csv\", header=True)\n",
    "bike_sharing.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- instant: string (nullable = true)\n",
      " |-- dteday: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- yr: string (nullable = true)\n",
      " |-- mnth: string (nullable = true)\n",
      " |-- holiday: string (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- workingday: string (nullable = true)\n",
      " |-- weathersit: string (nullable = true)\n",
      " |-- temp: string (nullable = true)\n",
      " |-- atemp: string (nullable = true)\n",
      " |-- hum: string (nullable = true)\n",
      " |-- windspeed: string (nullable = true)\n",
      " |-- casual: string (nullable = true)\n",
      " |-- registered: string (nullable = true)\n",
      " |-- cnt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bike_sharing.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = bike_sharing.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert string to integer or double by Using SQLTransformer\n",
    "\n",
    "### SQLTransformer implements the transformations which are defined by SQL statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform integer as int and flaot as double and the label\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "sql_transformer01 = SQLTransformer(\n",
    "    statement = \"\"\"\n",
    "    SELECT\n",
    "        cast(season as int),\n",
    "        cast(yr as int),\n",
    "        cast(mnth as int),\n",
    "        cast(holiday as int),\n",
    "        cast(weekday as int),\n",
    "        cast(workingday as int),\n",
    "        cast(weathersit as int),\n",
    "        cast(temp as double),\n",
    "        cast(atemp as double),\n",
    "        cast(hum as double),\n",
    "        cast(windspeed as double),\n",
    "        cast(cnt as int) as label\n",
    "    FROM __THIS__\n",
    "    \"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorAssembler Transformer\n",
    "VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models.\n",
    "VectorAssembler accepts the following input column types: all numeric types, boolean type, and vector type. In each row, the values of the input columns will be concatenated into a vector in the specified order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler()\\\n",
    "    .setInputCols([\"season\",\n",
    "                  \"yr\",\n",
    "                  \"mnth\",\n",
    "                  \"holiday\",\n",
    "                  \"weekday\",\n",
    "                  \"workingday\",\n",
    "                  \"weathersit\",\n",
    "                  \"temp\",\n",
    "                  \"atemp\",\n",
    "                  \"hum\",\n",
    "                  \"windspeed\"])\\\n",
    "    .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features and label by SQLTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select features and label only\n",
    "sql_transformer02 = SQLTransformer(\n",
    "    statement = \"\"\"\n",
    "    SELECT\n",
    "        features,\n",
    "        label\n",
    "    FROM __THIS__\n",
    "    \"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a linear regression in spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure an ML pipeline,\n",
    "which consists of three stages: SQL_transformer01, essembler, and SQL_transformer02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = [\n",
    "    sql_transformer01,\n",
    "    assembler,\n",
    "    sql_transformer02,\n",
    "    lr\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the pipeline to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline_model=pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2.0, 0.0, 4.0, 0.0, 4.0, 1.0, 1.0, 0.4675, 0....</td>\n",
       "      <td>3267</td>\n",
       "      <td>3922.355070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2.0, 0.0, 4.0, 0.0, 0.0, 0.0, 2.0, 0.581667, ...</td>\n",
       "      <td>4191</td>\n",
       "      <td>2941.055603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2.0, 0.0, 4.0, 0.0, 4.0, 1.0, 2.0, 0.6175, 0....</td>\n",
       "      <td>4058</td>\n",
       "      <td>3316.944245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  label   prediction\n",
       "0  [2.0, 0.0, 4.0, 0.0, 4.0, 1.0, 1.0, 0.4675, 0....   3267  3922.355070\n",
       "1  [2.0, 0.0, 4.0, 0.0, 0.0, 0.0, 2.0, 0.581667, ...   4191  2941.055603\n",
       "2  [2.0, 0.0, 4.0, 0.0, 4.0, 1.0, 2.0, 0.6175, 0....   4058  3316.944245"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train01 = pipeline_model.transform(train)\n",
    "test01= pipeline_model.transform(test)\n",
    "test01.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model (on the test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 = 0.795052189267716\n",
      "rmse = 874.8801605587502\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator()\n",
    "print (\"r2 =\", evaluator.evaluate(test01, {evaluator.metricName:\"r2\"}))\n",
    "print (\"rmse =\", evaluator.evaluate(test01, {evaluator.metricName:\"rmse\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the pipeline model for the production needs\n",
    "\n",
    "Often times it is worth it to save a model or a pipeline to disk for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the pipeline model for the production needs\n",
    "pipeline_model.save(\"pipeline_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the pipeline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the pipeline_model\n",
    "from pyspark.ml import PipelineModel\n",
    "new_pipeline_model = PipelineModel.load(\"pipeline_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 = 0.795052189267716\n",
      "rmse = 874.8801605587502\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test02 = new_pipeline_model.transform(test)\n",
    "print (\"r2 =\", evaluator.evaluate(test02, {evaluator.metricName:\"r2\"}))\n",
    "print (\"rmse =\", evaluator.evaluate(test02, {evaluator.metricName:\"rmse\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "more info :\n",
    "    - https://spark.apache.org/docs/latest/ml-features.html#sqltransformer    \n",
    "    - https://spark.apache.org/docs/2.2.0/ml-pipeline.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
